{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "class NumpyClassifier():\n",
    "    \"\"\"Common methods to \"\"\"\n",
    "\n",
    "    def accuracy(self, X_test, y_test, **kwargs):\n",
    "        pred = self.predict(X_test, **kwargs)\n",
    "        if len(pred.shape) > 1:\n",
    "            pred = pred[:, 0]\n",
    "        return sum(pred == y_test) / len(pred)\n",
    "\n",
    "\n",
    "class NumpyLogReg(NumpyClassifier):\n",
    "    def __init__(self, eta, lmb) -> None:\n",
    "        super().__init__()\n",
    "        self.eta = eta\n",
    "        self.lmb = lmb\n",
    "        \n",
    "    \n",
    "    def add_bias(self,X):\n",
    "        return np.c_[np.ones(X.shape[0])*(-1), X]\n",
    "\n",
    "    def fit(self, X_train, t_train,batch_size, epochs=30):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train are the targets values for training data\"\"\"\n",
    "        m = X_train.shape[1]\n",
    "        n_batches = np.ceil(X_train.shape[0] / batch_size)\n",
    "\n",
    "      \n",
    "        X_train = self.add_bias(X_train)\n",
    "        self.beta =  np.zeros(m + 1)\n",
    "        \n",
    "        indicies = np.arange(X_train.shape[0])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(indicies)\n",
    "            minibatches_idx = np.array_split(indicies, n_batches)\n",
    "            \n",
    "            for minibatch in range(len(minibatches_idx)):\n",
    "                xi = np.take(X_train, minibatches_idx[minibatch],axis=0)\n",
    "                yi = np.take(t_train, minibatches_idx[minibatch],axis=0)\n",
    "            \n",
    "                \n",
    "                self.newton_step(xi, yi)\n",
    "           \n",
    "\n",
    "    def forward(self, X):\n",
    "        return expit(X @ self.beta)\n",
    "    \n",
    "    def newton_step(self, X,y):\n",
    "        p = self.forward(X)\n",
    "        score = X.T @ (y-p)\n",
    "        \n",
    "        W = np.zeros(shape=(len(y),len(y)))\n",
    "        p_1 = 1-p\n",
    "        np.fill_diagonal(W,(p*p_1))\n",
    "        \n",
    "        hessian = -X.T@W@X\n",
    "        update = (np.linalg.pinv(hessian) @ score) + self.lmb*self.beta\n",
    "        self.beta -= self.eta * update\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        z = self.add_bias(x)\n",
    "        score = self.forward(z)\n",
    "        # score = z @ self.theta\n",
    "        return (score > threshold).astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data,cancer.target,test_size=0.25,random_state=4155)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eta_list = np.linspace(0.001,0.1,20)\n",
    "acu_scores = np.zeros(len(eta_list))\n",
    " \n",
    "cntr = 0\n",
    "for eta in eta_list:\n",
    "    logreg_test = NumpyLogReg(eta = eta, lmb = 1)\n",
    "    logreg_test.fit(X_train_scaled,y_train,batch_size= 5, epochs=50)\n",
    "    score = acu_scores[cntr] = logreg_test.accuracy(X_test_scaled, y_test)\n",
    "    cntr += 1 \n",
    "    \n",
    "    \n",
    "plt.plot(eta_list, acu_scores, label = \"LogReg accuracy\")\n",
    "plt.xlabel(\"Eta\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def sci_kit_test_acu(X_train, X_test, y_train, y_test):\n",
    "    logreg = LogisticRegression(penalty=\"l2\", C = 0.2)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    \n",
    "\n",
    "    return sum(y_pred == y_test) / len(y_pred)\n",
    "\n",
    "sci_kit_test_acu(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare neural network classification results with Logistic regression results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import os\n",
    "from common import *\n",
    "from models import NeuralNetwork, Layer, Fixed_layer, binary_classifier, relu, sigmoid\n",
    "\n",
    "print(f\"Root directory: {os.getcwd()}\")\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "    \"font.size\": 10,\n",
    "})\n",
    "\n",
    "#%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "hidden1_neurons = 50\n",
    "print(input_dim)\n",
    "hidden1 = Layer(nbf_inputs=input_dim, nbf_outputs=hidden1_neurons, name=\"hidden1\")\n",
    "output = Layer(nbf_inputs=hidden1.output, nbf_outputs=1, name=\"output\")\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.add_layer(hidden1)\n",
    "model.add_layer(output)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "#TODO: Had to reshape u_train(?)\n",
    "model.train_model(X_train_scaled, y_train.reshape(-1,1), batch_size=batch_size, epochs=epochs)\n",
    "y_hat = model.predict(X_test_scaled)\n",
    "print(y_hat)\n",
    "\n",
    "def accuracy(y_hat, y_true):\n",
    "\n",
    "    return sum(y_hat == y_true)# / len(y_hat)\n",
    "\n",
    "print(accuracy(y_hat,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20551ddceb59dc51fa628b42bb2a7289171df926f90359af355cededb82457a3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('in5520': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
