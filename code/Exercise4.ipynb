{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Ridge Regression on the Franke function with resampling (score 20 points)\n",
    "\n",
    "Write your own code for the Ridge method, either using matrix inversion or the singular value decomposition as done in the previous exercise. Perform the same bootstrap analysis as in the Exercise 2 (for the same polynomials) and the cross-validation in exercise 3 but now for different values of λ. Compare and analyze your results with those obtained in exercises 1-3. Study the dependence on λ.\n",
    "\n",
    "Study also the bias-variance trade-off as function of various values of the parameter λ. For the bias-variance trade-off, use the bootstrap resampling method. Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from common import *\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import os\n",
    "#import seaborn as sns\n",
    "\n",
    "print(f\"Root directory: {os.getcwd()}\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "    \"font.size\": 10,\n",
    "})\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Data\n",
    "Defining and creating the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate some data:\n",
    "SEED = 4155\n",
    "np.random.seed(SEED)\n",
    "n = 20\n",
    "x = np.sort(np.random.uniform(0, 1, n))\n",
    "y = np.sort(np.random.uniform(0, 1, n))\n",
    "x,y = np.meshgrid(x,y)\n",
    "t_nonoise = FrankeFunction(x, y)\n",
    "t = t_nonoise + noise_factor(n, factor=0.2)\n",
    "degree = 12\n",
    "min_lambda = -9\n",
    "max_lambda = 4\n",
    "nlambdas = 500\n",
    "lambdas = np.logspace(min_lambda,max_lambda, nlambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 initial search landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = np.zeros((degree, nlambdas))\n",
    "train_mse = np.zeros_like(test_mse)\n",
    "model_list = np.empty_like(test_mse, dtype=object)\n",
    "optimal_deg = 0\n",
    "optimal_lmb = 0\n",
    "lambda_degree = 0\n",
    "best_mse = np.inf\n",
    "\n",
    "for deg in range(1, degree+1):\n",
    "    for lmb in range(len(lambdas)):\n",
    "\n",
    "        X = create_X(x,y,n=deg)\n",
    "\n",
    "        X_train, X_test, z_train, z_test = prepare_data(X, t.ravel(), SEED, scale_X=True, skip_intercept=True)\n",
    "\n",
    "        model = RidgeRegression(lambdas[lmb])\n",
    "        z_hat_train = model.fit(X_train, z_train)\n",
    "        z_hat_test = model.predict(X_test)\n",
    "\n",
    "        test_mse[deg-1,lmb] = MSE(z_test, z_hat_test)\n",
    "        train_mse[deg-1, lmb] = MSE(z_train, z_hat_train)\n",
    "        model_list[deg-1,lmb] = model\n",
    "        \n",
    "        if test_mse[deg-1,lmb] < best_mse:\n",
    "            best_mse = test_mse[deg-1, lmb]\n",
    "            optimal_deg = deg\n",
    "            optimal_lmb = lambdas[lmb]\n",
    "            lambda_degree = lmb\n",
    "\n",
    "print(best_mse)\n",
    "print(optimal_lmb)\n",
    "print(optimal_deg)\n",
    "print(train_mse[deg-1, lmb])\n",
    "optimal_model = model_list[deg-1, lambda_degree]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surface-plot of optimal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mticker\n",
    "%matplotlib\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.title.set_text(\"Plot of the Search Landscape\")\n",
    "ax.set_xlabel(\"Polynomial degree\"); ax.set_ylabel(\"Lambda index\"); ax.set_zlabel(\"MSE\")\n",
    "#ax.set_yticklabels(f\"{np.log10(lambdas[i])}\" for i in np.linspace(0,499,6,dtype=int))\n",
    "ax.set_xticklabels(f\"{deg-2}\" for deg in range(1, degree+2,2))\n",
    "\n",
    "degs, lambs = np.meshgrid(range(degree), range(nlambdas))\n",
    "print(degs.shape)\n",
    "print(lambs.shape)\n",
    "print(test_mse.shape)\n",
    "surf = ax.plot_surface(degs, lambs, test_mse.swapaxes(0,1), cmap=cm.coolwarm)\n",
    "ax.view_init(elev=14., azim=-58.)\n",
    "ax.scatter(optimal_deg-1, lambda_degree, best_mse, c='r', marker='o', s=100)\n",
    "# plt.savefig(f\"{REPORT_FIGURES}{EX4}search_landscape_ridge.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betaplot showing effect of ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for deg in range(1, degree-3):     # Only want the first 8\n",
    "    summaries_df = pd.DataFrame()\n",
    "    for lmb in lambdas:\n",
    "        X = create_X(x,y,n=deg)\n",
    "        X_train, X_test, z_train, z_test = prepare_data(X, t.ravel(), 4155, scale_X=True, skip_intercept=True)\n",
    "        model = RidgeRegression(lmb)\n",
    "        model.fit(X_train, z_train)\n",
    "        summary_df = model.summary()\n",
    "        summaries_df = pd.concat([summaries_df, summary_df], axis=0)\n",
    "\n",
    "    fig = plot_beta_errors_for_lambdas(summaries_df, deg)\n",
    "    # plt.savefig(f\"{REPORT_FIGURES}{EX4}beta_plot_ridge_{deg}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code that implements the brute force lambda selection, this is inspired by the lecture 30.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NOISE\n",
    "\n",
    "X = create_X(x,y,degree)\n",
    "X_train, X_test, t_train, t_test = prepare_data(X,t_nonoise.ravel(),SEED, scale_X=True, skip_intercept=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "MSERidgePredict = np.zeros(nlambdas)\n",
    "MSEOurRidge = np.zeros(nlambdas)\n",
    "lambdas = np.logspace(min_lambda, max_lambda, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "\n",
    "    # SKlearn\n",
    "    RegRidge = lm.Ridge(lmb,fit_intercept=False) # ALWAYS keep intercept False\n",
    "    RegRidge.fit(X_train, t_train)\n",
    "    tpredictRidge = RegRidge.predict(X_test)\n",
    "    MSERidgePredict[i] = MSE(t_test, tpredictRidge)\n",
    "\n",
    "    # our Ridge\n",
    "    model = RidgeRegression(lmb)\n",
    "    model.fit(X_train, t_train)\n",
    "    tpredictOur = model.predict(X_test)\n",
    "    MSEOurRidge[i] = MSE(t_test, tpredictOur)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), MSEOurRidge, 'm', label = \"MSE Our Ridge Test\")\n",
    "plt.plot(np.log10(lambdas), MSERidgePredict, 'y--', label = 'MSE SL Ridge Test')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOISE and OLS for comparison\n",
    "\n",
    "X = create_X(x,y,degree)\n",
    "X_train, X_test, t_train, t_test = prepare_data(X,t.ravel(), SEED, scale_X=True, skip_intercept=True)\n",
    "\n",
    "\n",
    "nlambdas = 500\n",
    "MSERidgePredict = np.zeros(nlambdas)\n",
    "MSEOurRidge = np.zeros(nlambdas)\n",
    "MSEols = np.zeros(nlambdas)\n",
    "lambdas = np.logspace(min_lambda, max_lambda, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "\n",
    "    # SKlearn\n",
    "    RegRidge = lm.Ridge(lmb,fit_intercept=False) # ALWAYS keep intercept False\n",
    "    RegRidge.fit(X_train, t_train)\n",
    "    tpredictRidge = RegRidge.predict(X_test)\n",
    "    MSERidgePredict[i] = MSE(t_test, tpredictRidge)\n",
    "\n",
    "    # our Ridge\n",
    "    model = RidgeRegression(lmb)\n",
    "    model.fit(X_train, t_train)\n",
    "    tpredictOur = model.predict(X_test)\n",
    "    MSEOurRidge[i] = MSE(t_test, tpredictOur)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), MSEOurRidge, 'm', label = \"MSE Our Ridge Test\")\n",
    "plt.plot(np.log10(lambdas), MSERidgePredict, 'y--', label = 'MSE SL Ridge Test')\n",
    "#plt.plot(np.log10(lambdas), MSEols, 'b--', label = \"MSE Our OLS Test\")\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping\n",
    "looping through a fewer set of lambda values, still from -9 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "maxdegree = 18\n",
    "n_points = [20, 40]\n",
    "x = np.sort(np.random.uniform(0, 1, n))\n",
    "y = np.sort(np.random.uniform(0, 1, n))\n",
    "x,y = np.meshgrid(x,y)\n",
    "z = FrankeFunction(x,y) + noise_factor(n, factor=0.2)\n",
    "\n",
    "min_lambda = -9\n",
    "max_lambda = 4\n",
    "nlambdas = 13\n",
    "lambdas = np.logspace(min_lambda,max_lambda, nlambdas+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in n_points:\n",
    "    for i in [0, len(lambdas)-1]:\n",
    "        n_bootstraps = int(0.2*n**2)\n",
    "        lmb = lambdas[i]\n",
    "        polydegree = np.arange(1, maxdegree+1)\n",
    "        MSE_test, MSE_train, bias, variance = bootstrap(x, y, z, maxdegree, n_bootstraps, RidgeRegression(lmb), SEED)\n",
    "\n",
    "        plt.plot(polydegree, MSE_test,\"m\", label='MSE test')\n",
    "        #plt.plot(polydegree, MSE_train,\"c\", label='MSE train')\n",
    "        plt.plot(polydegree, bias,\"b--\", label='bias')\n",
    "        plt.plot(polydegree, variance,\"r--\", label='Variance')\n",
    "        #plt.plot(polydegree, bias+variance,\"g--\", label='bias+variance')\n",
    "\n",
    "        plt.title(fr\"Bias-Variance tradeoff for {n*n} datapoints using {n_bootstraps} bootstraps with $\\lambda$: {lmb:.0e}\")\n",
    "        plt.xlabel(\"Model complexity / Polynomial Degree\")\n",
    "        plt.ylabel(\"Prediction Error - MSE\")\n",
    "        plt.xticks(polydegree)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        # plt.savefig(f\"{REPORT_FIGURES}{EX4}ridge_complexity_using_bootstrap_function_lmb{n}{i}.pdf\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation using Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "maxdegree = 12\n",
    "n = 20\n",
    "x = np.sort(np.random.uniform(0, 1, n))\n",
    "y = np.sort(np.random.uniform(0, 1, n))\n",
    "x,y = np.meshgrid(x,y)\n",
    "z = FrankeFunction(x,y) + noise_factor(n, factor=0.2)\n",
    "\n",
    "min_lambda = -9\n",
    "max_lambda = 4\n",
    "nlambdas = 3\n",
    "lambdas = np.logspace(min_lambda,max_lambda, nlambdas+1)\n",
    "\n",
    "for i in range(nlambdas+1):\n",
    "    lmb = lambdas[i]\n",
    "\n",
    "    boot_strp_MSE_test, _, _, boot_strp_variance = bootstrap(x, y, z, maxdegree, int(0.2*n**2), RidgeRegression(lmb), SEED)\n",
    "    boot_strp_std = np.sqrt(boot_strp_variance)\n",
    "\n",
    "    for degree in range(3,maxdegree):\n",
    "        X = create_X(x,y,degree)\n",
    "        X = remove_intercept(X)\n",
    "\n",
    "        mean_folds_error = np.zeros(6)\n",
    "        mse_std_arr = np.zeros(6)\n",
    "        for folds in range(5,11):\n",
    "        \n",
    "            implemented_scores = cross_val(k = folds, model = \"Ridge\", X = X, z = t, degree=degree, shuffle=True, scale_t=False, lmb=lmb)\n",
    "            mean_folds_error[folds-5] = np.mean(implemented_scores)\n",
    "            mse_std_arr[folds-5] = np.std(implemented_scores)\n",
    "    \n",
    "        plt.plot(np.arange(5,11), np.ones(6)*boot_strp_MSE_test[degree],\"--\", label =\"Mean MSE bootstrap with STD\")\n",
    "        #plt.fill_between(np.arange(5,11), np.ones(6)*boot_strp_MSE_test[degree]-boot_strp_std[degree],\n",
    "                        #np.ones(6)*boot_strp_MSE_test[degree]+boot_strp_std[degree], alpha = 0.2 )\n",
    "        plt.plot(np.arange(5,11), mean_folds_error, \"o--\",  label = \"Mean MSE CV\")\n",
    "        plt.fill_between(np.arange(5,11), mean_folds_error-mse_std_arr, mean_folds_error+mse_std_arr,  alpha = 0.2, color = \"darkorange\")\n",
    "        plt.title(fr\"Model complexity: {degree} degrees, $\\lambda$ = {lmb:.2e}\")\n",
    "        plt.xlabel(\"K-fold\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        #plt.ylim(0,2)\n",
    "        plt.xticks(np.arange(5,11))\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        # plt.savefig(f\"{REPORT_FIGURES}{EX4}mse_cv_boot{i}{degree}.pdf\")\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20551ddceb59dc51fa628b42bb2a7289171df926f90359af355cededb82457a3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('in5520': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
