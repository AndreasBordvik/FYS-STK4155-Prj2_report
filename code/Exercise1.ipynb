{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from common import *\n",
    "import pandas as pd\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn.linear_model as lm\n",
    "#\n",
    "\n",
    "print(f\"Root directory: {os.getcwd()}\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "    \"font.size\": 10,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Franke function 3D preview\n",
    "First we plot a 3D plot of the franke function.\n",
    "The plot is based on the provided code in the assignmentext for plotting the franke function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview plot of the franke function\n",
    "#%matplotlib\n",
    "SEED_VALUE = np.random.seed(4155)\n",
    "n = 40\n",
    "y = x = np.sort(np.random.uniform(0,1,n))\n",
    "x, y = np.meshgrid(x,y)\n",
    "z = FrankeFunction(x, y)\n",
    "z_noisy = z + noise_factor(n, factor=0.2)\n",
    "\n",
    "fig = plt.figure()\n",
    "# Ploting frankefunction without noise\n",
    "ax1 = fig.add_subplot(111, projection='3d') # Are :)steike\n",
    "ax1.title.set_text(\"Plot of the Franke Function\")\n",
    "ax1.view_init(elev=30., azim=-25.0)\n",
    "ax1.set_xlabel(\"x\"); ax1.set_ylabel(\"y\"); ax1.set_zlabel(\"z\")\n",
    "surf1 = ax1.plot_surface(x,y,z, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n",
    "# Customize the z axis.\n",
    "ax1.set_zlim(-0.10, 1.40)\n",
    "ax1.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax1.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "# plt.savefig(f\"{REPORT_FIGURES}{EX1}franke_function_nonoise_preview.pdf\")\n",
    "\n",
    "# Ploting frankefunction with noise\n",
    "fig = plt.figure()\n",
    "ax2 = fig.add_subplot(111, projection='3d')\n",
    "ax2.title.set_text(\"Plot of the Franke Function\\n(0.2*Gaussian Noise added)\")\n",
    "ax2.view_init(elev=30., azim=-25.0)\n",
    "ax2.set_xlabel(\"x\"); ax2.set_ylabel(\"y\"); ax2.set_zlabel(\"z\")\n",
    "surf2 = ax2.plot_surface(x,y,z_noisy, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n",
    "# Customize the z axis.\n",
    "ax2.set_zlim(-0.10, 1.40)\n",
    "ax2.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax2.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "# plt.savefig(f\"{REPORT_FIGURES}{EX1}franke_function_noise_preview.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Ordinary Least Squeares (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data\n",
    "Defining and creating the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_VALUE = np.random.seed(4155)\n",
    "n = 100 # The number of points in direction for the Franke Function\n",
    "x = np.sort(np.random.uniform(0, 1, n))\n",
    "y = np.sort(np.random.uniform(0, 1, n))\n",
    "x, y = np.meshgrid(x,y)\n",
    "z = FrankeFunction(x, y) + noise_factor(n,factor=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Plot of fit for all degrees before evaluation\n",
    " We plot the fit up to degree 6 to get an intuition on the curvature of the fitted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "degrees = 6\n",
    "z_train_OLS = pd.DataFrame()\n",
    "z_hat_train_OLS = pd.DataFrame()\n",
    "z_test_OLS = pd.DataFrame()\n",
    "z_hat_test_OLS = pd.DataFrame()\n",
    "\n",
    "# TODO: Must fix so that training and test data are used. \n",
    "# Must evalute model using MSE from traning and test\n",
    "for degree in range(1, degrees + 1):\n",
    "    X = create_X(x, y, degree) # Design MatrixS\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z.ravel(), SEED_VALUE, scale_X=True, skip_intercept=True)\n",
    "    model = OLS() # The model\n",
    "    #model.fit(X, z) # Fitting the model\n",
    "    z_hat_train = model.fit(X_train, z_train) # Fitting the model\n",
    "    z_hat = model.predict(X[:,1:]) # predict on train data\n",
    "    \n",
    "    # Plot\n",
    "    ax = fig.add_subplot(3,2, degree, projection='3d')\n",
    "    ax.view_init(elev=30., azim=-25.0)\n",
    "    ax.title.set_text(f\"OLS/Linear fit of degree{degree}\")\n",
    "    ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\n",
    "    ax.scatter3D(y, x, z_hat, c=z_hat ,marker = '.', cmap=cm.coolwarm)\n",
    "fig.suptitle(\"OLS fit to the Franke Function\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{REPORT_FIGURES}{EX1}franke_function_OLS_fit.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Finding degree/model complexity for the optimal OLS fit\n",
    "Approximate the franke function using ordinary least squares\n",
    "We estimate the franke functinon using polynomials up to 6th degree. We than look at the MSE scores to look for overfitting. We use the MSE score values from the test data to determine overfit together with the curvature of the evaluation plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervall \n",
    "$$CI_{0.95}(\\hat\\beta_i) = [\\hat\\beta_i-1.96 SE(\\hat\\beta_i), \\hat\\beta_i+1.96 SE(\\hat\\beta_i)] =\\hat\\beta_i \\pm 1.96\\hat \\sigma(\\hat\\beta_i)$$ \n",
    "In order to estimate the variance of the $i$-th beta values: $$\\sigma^2 (\\beta_i ) = \\sigma^2 [(X^{T} X)^{-1}]_{ii}$$\n",
    "However, $\\sigma$ is unkown and can be generaly estimated as followed:\n",
    "$$\\hat\\sigma^2 = \\frac{\\sum_{i=0}^{N-1}(y_i - \\hat y_i)^2}{N}$$\n",
    "For simplification purposes, we N instead of N-p-1 in the denominator.<br>\n",
    "To get the variance estimate of each $\\beta$ component one must calculate the variance with respect to the diagonal elements of $(X^TX)^{-1}$ Estimated standard error is the square root of $\\hat\\sigma^2$, where the estimate for variance $\\hat\\sigma^2$ is:\n",
    "$$\\hat\\sigma^2 = \\frac{\\sum_{i=0}^{N-1}(y_i - \\hat y_i)^2}{N}(X^TX)^{-1}$$\n",
    "Where y is the true value, and $\\hat y$ being the predicted value. <br>\n",
    "The variance estimate of each $\\hat\\beta$ estimate can be written as:\n",
    "$$\\hat\\sigma_{\\hat\\beta_i}^2 = \\hat\\sigma^2(X^TX)_{i,i}^{-1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_VALUE = np.random.seed(4155)\n",
    "n = 20 # The number of points in direction for the Franke Function\n",
    "x = np.sort(np.random.uniform(0, 1, n))\n",
    "y = np.sort(np.random.uniform(0, 1, n))\n",
    "x, y = np.meshgrid(x,y)\n",
    "noise = 0.05\n",
    "z = FrankeFunction(x, y) + noise_factor(n,factor=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = 5\n",
    "\n",
    "# Setting up dataframes for the observed values\n",
    "z_train_OLS = pd.DataFrame()\n",
    "z_test_OLS = pd.DataFrame()\n",
    "\n",
    "# Setting up the dataframes for our computed values\n",
    "z_hat_train_OLS = pd.DataFrame()\n",
    "z_hat_test_OLS = pd.DataFrame()\n",
    "\n",
    "# Setting up the dataframes for our SVD computed values\n",
    "z_hat_train_SVD = pd.DataFrame()\n",
    "z_hat_test_SVD = pd.DataFrame()\n",
    "\n",
    "# Setting up dataframes for sklearn computed values\n",
    "z_hat_train_SK = pd.DataFrame()\n",
    "z_hat_test_SK = pd.DataFrame()\n",
    "\n",
    "coeffs_df = pd.DataFrame()\n",
    "\n",
    "for degree in range(1, degrees+1):\n",
    "    X = create_X(x, y, degree) # Design Matrix\n",
    "    \n",
    "    # Scaling data and splitting it into training and test sets\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z.ravel(), test_size=0.2, shuffle=True, scale_X=True, scale_t=False, skip_intercept=True, random_state=SEED_VALUE)\n",
    "    \n",
    "    # Model construction, fitting, and predictions using matrix inversion\n",
    "    model = OLS() # The model\n",
    "    z_hat_train = model.fit(X_train, z_train) # Fitting the model and predict on training data\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\n",
    "\n",
    "   \n",
    "\n",
    "    # Model construction, fitting and predictions using sklearn\n",
    "    model_sk = lm.LinearRegression(fit_intercept=False)\n",
    "    model_sk.fit(X_train, z_train)\n",
    "    z_hat_train_sk = model_sk.predict(X_train)\n",
    "    z_hat_test_sk = model_sk.predict(X_test)\n",
    "    \n",
    "    # Evaluatation metrics OLS, SVD, SK\n",
    "    MSE_score_train = MSE(z_train, z_hat_train)\n",
    "    R2_score_train = R2(z_train, z_hat_train)\n",
    "    MSE_score_test = MSE(z_test, z_hat_test)\n",
    "    R2_score_test = R2(z_test, z_hat_test)\n",
    "\n",
    "    MSE_score_train_sk = MSE(z_train, z_hat_train_sk)\n",
    "    R2_score_train_sk = R2(z_train, z_hat_train_sk)\n",
    "    MSE_score_test_sk = MSE(z_test, z_hat_test_sk)\n",
    "    R2_score_test_sk = R2(z_test, z_hat_test_sk)\n",
    "    \n",
    "    # Estimated standard error for the beta coefficients\n",
    "    SE_betas = model.SE\n",
    "\n",
    "    var_hat = (1/X_train.shape[0]) * np.sum((z_train - z_hat_train)**2) # Estimated variance\n",
    "\n",
    "\n",
    "    # Calculating 95% confidence intervall OLS, SVD\n",
    "    betas = model.get_all_betas\n",
    "    CI_lower_all_betas = betas - (1.96 * SE_betas)\n",
    "    CI_upper_all_betas = betas + (1.96 * SE_betas)\n",
    "    CL = np.zeros((CI_upper_all_betas.shape[0],2))\n",
    "    CL[:,0] = CI_lower_all_betas\n",
    "    CL[:,1] = CI_upper_all_betas\n",
    "\n",
    "    betas_svd = model.get_all_betas\n",
    "\n",
    "    # Constructing dataframe for beta coefficients\n",
    "    degs = np.zeros(betas.shape[0]); degs.fill(degree)\n",
    "    df = pd.DataFrame.from_dict({\"degree\" :degs,\n",
    "                                 \"coeff_name\": [f\"b{i}\" for i in range(1,betas.shape[0]+1)],\n",
    "                                 \"coeff value\": np.round(betas, decimals=4),\n",
    "                                 \"std error\": np.round(SE_betas, decimals=4),\n",
    "                                 \"CI_lower\":np.round(CI_lower_all_betas, decimals=4), \n",
    "                                 \"CI_upper\":np.round(CI_upper_all_betas, decimals=4)},\n",
    "                                 orient='index').T\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    coeffs_df = pd.concat([coeffs_df,df], axis=0)\n",
    "\n",
    "\n",
    "        \n",
    "    # Filling up dataframes for train and test evaluation\n",
    "    z_train_OLS[degree] = z_train.flatten() \n",
    "    z_hat_train_OLS[degree] = z_hat_train.flatten()\n",
    "    z_test_OLS[degree] = z_test.flatten()\n",
    "    z_hat_test_OLS[degree] = z_hat_test.flatten()\n",
    "\n",
    "    z_hat_train_SK[degree] = z_hat_train_sk.flatten()\n",
    "    z_hat_test_SK[degree] = z_hat_test_sk.flatten()\n",
    "\n",
    "\n",
    "# MSE calculations for all lambda values\n",
    "mse_scores_train = ((z_train_OLS - z_hat_train_OLS) ** 2).mean()\n",
    "mse_scores_test = ((z_test_OLS - z_hat_test_OLS) ** 2).mean()\n",
    "\n",
    "mse_scores_train_svd = ((z_train_OLS - z_hat_train_SVD) ** 2).mean()\n",
    "mse_scores_test_svd = ((z_test_OLS - z_hat_test_SVD) ** 2).mean()\n",
    "\n",
    "mse_scores_train_sk = ((z_train_OLS - z_hat_train_SK) ** 2).mean()\n",
    "mse_scores_test_sk = ((z_test_OLS - z_hat_test_SK) ** 2).mean()\n",
    "\n",
    "# R2 calculations for all lambda values\n",
    "R2_scores_train = 1 - ((z_train_OLS - z_hat_train_OLS) ** 2).sum() / ((z_train_OLS - z_train_OLS.mean())**2).sum() \n",
    "R2_scores_test = 1 - ((z_test_OLS - z_hat_test_OLS) ** 2).sum() / ((z_test_OLS - z_test_OLS.mean())**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train, c=\"c\", label=\"Training data\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test, c=\"m\", label=\"Test data\")\n",
    "#plt.plot(np.arange(1,degrees+1), mse_scores_train_svd, \"b--\", label=\"Training data SVD\")\n",
    "#plt.plot(np.arange(1,degrees+1), mse_scores_test_svd, \"g--\", label=\"Test data SVD\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train_sk, \"r--\", label=\"Training data sklearn\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test_sk, \"y--\", label=\"Test data sklearn\")\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\n",
    "plt.ylabel(\"Prediction Error - MSE\")\n",
    "plt.title(f\"Training evaluation on OLS regression fit\\n(noise factor {noise})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{REPORT_FIGURES}{EX1}franke_function_OLS_evaluate_fit_1.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at $\\beta$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,degree+1):\n",
    "    degree_coeffs = coeffs_df[coeffs_df['degree'] == i]\n",
    "    display(degree_coeffs)\n",
    "    fig = plot_beta_errors(degree_coeffs, i)\n",
    "    plt.title(f\"Beta error OLS - degree{degree}\\n(noise factor {noise})\")\n",
    "    # fig.savefig(f\"{REPORT_FIGURES}{EX1}OLS_beta_error_degree{i}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the subpar Figure labels, we have not been able to determine when they were introduced. We refer to the report pdf for a earlier and correct rendition of the plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat of the previous exercise, this time with foure times the amount of added noise. (0.05 -> 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_VALUE = np.random.seed(4155)\n",
    "degrees = 5\n",
    "n = 20 # The number of points in direction for the Franke Function\n",
    "x = np.sort(np.random.uniform(0, 1, n))\n",
    "y = np.sort(np.random.uniform(0, 1, n))\n",
    "x, y = np.meshgrid(x,y)\n",
    "noise = 0.2\n",
    "z = FrankeFunction(x, y) + noise_factor(n,factor=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up dataframes for the observed values\n",
    "z_train_OLS = pd.DataFrame()\n",
    "z_test_OLS = pd.DataFrame()\n",
    "\n",
    "# Setting up the dataframes for our computed values\n",
    "z_hat_train_OLS = pd.DataFrame()\n",
    "z_hat_test_OLS = pd.DataFrame()\n",
    "\n",
    "# Setting up dataframes for sklearn computed values\n",
    "z_hat_train_SK = pd.DataFrame()\n",
    "z_hat_test_SK = pd.DataFrame()\n",
    "\n",
    "coeffs_df = pd.DataFrame()\n",
    "\n",
    "for degree in range(1, degrees+1):\n",
    "    X = create_X(x, y, degree) # Design Matrix\n",
    "    \n",
    "    # Scaling data and splitting it into training and test sets\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z.ravel(), SEED_VALUE, test_size=0.2, shuffle=True, scale_X=True, skip_intercept=True)\n",
    "    \n",
    "    # Model construction, fitting, and predictions using matrix inversion\n",
    "    model = OLS() # The model\n",
    "    z_hat_train = model.fit(X_train, z_train) # Fitting the model and predict on training data\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\n",
    "\n",
    "    # Model construction, fitting and predictions using sklearn\n",
    "    model_sk = lm.LinearRegression(fit_intercept=False)\n",
    "    model_sk.fit(X_train, z_train)\n",
    "    z_hat_train_sk = model_sk.predict(X_train)\n",
    "    z_hat_test_sk = model_sk.predict(X_test)\n",
    "    \n",
    "    # Evaluatation metrics OLS, SVD, SK\n",
    "    MSE_score_train = MSE(z_train, z_hat_train)\n",
    "    R2_score_train = R2(z_train, z_hat_train)\n",
    "    MSE_score_test = MSE(z_test, z_hat_test)\n",
    "    R2_score_test = R2(z_test, z_hat_test)\n",
    "\n",
    "\n",
    "    MSE_score_train_sk = MSE(z_train, z_hat_train_sk)\n",
    "    R2_score_train_sk = R2(z_train, z_hat_train_sk)\n",
    "    MSE_score_test_sk = MSE(z_test, z_hat_test_sk)\n",
    "    R2_score_test_sk = R2(z_test, z_hat_test_sk)\n",
    "    \n",
    "    # Estimated standard error for the beta coefficients\n",
    "    SE_betas = model.SE\n",
    "    \n",
    "    # Calculating 95% confidence intervall OLS, SVD\n",
    "    \n",
    "    betas = model.get_all_betas\n",
    "    CI_lower_all_betas = betas - (1.96 * SE_betas)\n",
    "    CI_upper_all_betas = betas + (1.96 * SE_betas)\n",
    "    CL = np.zeros((CI_upper_all_betas.shape[0],2))\n",
    "    CL[:,0] = CI_lower_all_betas\n",
    "    CL[:,1] = CI_upper_all_betas\n",
    "\n",
    "    betas_svd = model.get_all_betas\n",
    "\n",
    "    \n",
    "    # Constructing dataframe for beta coefficients\n",
    "    degs = np.zeros(betas.shape[0]); degs.fill(degree)\n",
    "    df = pd.DataFrame.from_dict({\"degree\" :degs,\n",
    "                                 \"coeff_name\": [f\"b{i}\" for i in range(1,betas.shape[0]+1)],\n",
    "                                 \"coeff value\": np.round(betas, decimals=4),\n",
    "                                 \"std error\": np.round(SE_betas, decimals=4),\n",
    "                                 \"CI_lower\":np.round(CI_lower_all_betas, decimals=4), \n",
    "                                 \"CI_upper\":np.round(CI_upper_all_betas, decimals=4)},\n",
    "                                 orient='index').T\n",
    "    coeffs_df = pd.concat([coeffs_df,df], axis=0)\n",
    "    \n",
    "        \n",
    "    # Filling up dataframes for train and test evaluation\n",
    "    z_train_OLS[degree] = z_train.flatten() \n",
    "    z_hat_train_OLS[degree] = z_hat_train.flatten()\n",
    "    z_test_OLS[degree] = z_test.flatten()\n",
    "    z_hat_test_OLS[degree] = z_hat_test.flatten()\n",
    "    z_hat_train_SK[degree] = z_hat_train_sk.flatten()\n",
    "    z_hat_test_SK[degree] = z_hat_test_sk.flatten()\n",
    "# MSE calculations for all lambda values\n",
    "mse_scores_train = ((z_train_OLS - z_hat_train_OLS) ** 2).mean()\n",
    "mse_scores_test = ((z_test_OLS - z_hat_test_OLS) ** 2).mean()\n",
    "\n",
    "mse_scores_train_sk = ((z_train_OLS - z_hat_train_SK) ** 2).mean()\n",
    "mse_scores_test_sk = ((z_test_OLS - z_hat_test_SK) ** 2).mean()\n",
    "# R2 calculations for all lambda values\n",
    "R2_scores_train = 1 - ((z_train_OLS - z_hat_train_OLS) ** 2).sum() / ((z_train_OLS - z_train_OLS.mean())**2).sum() \n",
    "R2_scores_test = 1 - ((z_test_OLS - z_hat_test_OLS) ** 2).sum() / ((z_test_OLS - z_test_OLS.mean())**2).sum()\n",
    "\n",
    "Morten_r2_train = R2(z_train_OLS, z_hat_train_OLS)\n",
    "Morten_r2_test = R2(z_test_OLS, z_hat_test_OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train, c=\"c\", label=\"Training data\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test, c=\"m\", label=\"Test data\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train_sk, \"r--\", label=\"Training data sklearn\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test_sk, \"y--\", label=\"Test data sklearn\")\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\n",
    "plt.ylabel(\"Prediction Error - MSE\")\n",
    "plt.title(f\"Training evaluation on OLS regression fit\\n(noise factor {noise})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{REPORT_FIGURES}{EX1}franke_function_OLS_evaluate_fit_dn.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,degrees+1), R2_scores_test, \"r--\", label=\"Test data\")\n",
    "plt.plot(np.arange(1,degrees+1), R2_scores_train, \"y--\", label=\"Training data\")\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\n",
    "plt.ylabel(\"R2 score\")\n",
    "plt.title(f\"R2 evaluation on OLS regression fit\\n(noise factor {noise})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{REPORT_FIGURES}{EX1}franke_function_R2.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,degree+1):\n",
    "    degree_coeffs = coeffs_df[coeffs_df['degree'] == i]\n",
    "    display(degree_coeffs)\n",
    "    fig = plot_beta_errors(degree_coeffs, i)\n",
    "    plt.title(f\"Beta error OLS - degree{degree}\\n(noise factor {noise})\")\n",
    "    # fig.savefig(f\"{REPORT_FIGURES}{EX1}OLS_beta_error_degree{i}_mn.pdf\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
