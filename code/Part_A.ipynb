{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part a) SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import autograd.numpy as np\n",
    "import pandas as pd\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import os\n",
    "from common import *\n",
    "from models import own_LinRegGD\n",
    "import cv2\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from autograd import grad\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "print(f\"Root directory: {os.getcwd()}\")\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "    \"font.size\": 10,\n",
    "})\n",
    "\n",
    "#%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_VALUE = 70707070\n",
    "np.random.seed(SEED_VALUE)\n",
    "SAVE_FIGURES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data and resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the terrain\n",
    "terrain1_file = \"SRTM_data_Norway_1.tif\"\n",
    "terrain2_file = \"SRTM_data_Norway_2.tif\"\n",
    "terrain1 =  imread(f'{INPUT_DATA}{terrain1_file}')\n",
    "terrain2 = imread(f'{INPUT_DATA}{terrain2_file}')\n",
    "\n",
    "# Resizing the image\n",
    "rescale_factor = 0.1\n",
    "y_size = int(terrain1.shape[0] * rescale_factor)\n",
    "x_size = int(terrain1.shape[1] * rescale_factor)\n",
    "terrain1Resized = cv2.resize(terrain1, (x_size, y_size))\n",
    "terrain2Resized = cv2.resize(terrain2, (x_size, y_size))\n",
    "\n",
    "# Plotting terrain\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.title.set_text(\"Terrain over Norway 1 (Resized)\")\n",
    "ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\n",
    "surf1 = ax1.imshow(terrain1Resized, cmap='gray')\n",
    "ax2.title.set_text(\"Terrain over Norway 2 (Resized)\")\n",
    "ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\")\n",
    "surf2 = ax2.imshow(terrain2Resized, cmap='gray')\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(f\"{REPORT_FIGURES}{EX_A}terrain_data_resized.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating image patches and Terrain data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nXpatches = 3; nYpatches=6\n",
    "y_steps = int(terrain2Resized.shape[0] / nYpatches); print(y_steps)\n",
    "x_steps = int(terrain2Resized.shape[1] / nXpatches); print(x_steps)\n",
    "\n",
    "patches_1 = create_img_patches(terrain1Resized, y_steps, x_steps)\n",
    "if SAVE_FIGURES:\n",
    "    fig1 = plotTerrainPatches(patches_1, nYpatches, nXpatches, plotTitle=\"Terrain1 patches\")\n",
    "    plt.savefig(f\"{REPORT_FIGURES}{EX_A}Terrain1_patches.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "patches_2 = create_img_patches(terrain2Resized, y_steps, x_steps)\n",
    "if SAVE_FIGURES:\n",
    "    fig2 = plotTerrainPatches(patches_2, nYpatches, nXpatches, plotTitle=\"Terrain2 patches\")\n",
    "    plt.savefig(f\"{REPORT_FIGURES}{EX_A}Terrain2_patches.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "# Choosing two interesting terrain patches\n",
    "img1 = patches_1[2]\n",
    "img2 = patches_2[5]\n",
    "x1, y1, z1 = createTerrainData(img1)\n",
    "x2, y2, z2 = createTerrainData(img2)\n",
    "\n",
    "# Constructing the terrain data\n",
    "terrain_data = 1\n",
    "if terrain_data == 1: # Choosing terrain1*\n",
    "    x, y, z = x1, y1, z1.copy() \n",
    "    #z_min = np.min(z)\n",
    "    z_max = np.max(z)\n",
    "    z = z1\n",
    "\n",
    "elif terrain_data == 2: # Choosing terrain2\n",
    "    x, y, z = x2, y2, z2.copy() \n",
    "    #z_min = np.min(z)\n",
    "    z_max = np.max(z)\n",
    "    z = z2\n",
    "    \n",
    "z_flat = z.ravel(); z_flat = z_flat.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function, with autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_MSE(X,y,theta, lmb=0):\n",
    "    return ((y - X @ theta)**2).sum() + lmb*(theta**2).sum()\n",
    "\n",
    "d_cost_MSE = grad(cost_MSE, 2)\n",
    "\n",
    "\n",
    "def cost_CE(X,y,theta, lmb=0):\n",
    "    pass\n",
    "\n",
    "d_cost_CE = grad(cost_CE, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning scheduler inspired my ~~Morten~~ SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time based learning-rate scheduler, smaller steps as time moves forward\n",
    "def learning_scheduler(t, alpha = 1):\n",
    "    return 1./(alpha * (t + t0))\n",
    "\n",
    "def step_length(t, t0, t1):\n",
    "    return t0/(t+t1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_sgd(X_train, t_train, theta, n_epoch, batch_size, eta, lr_scheduler=False, lmb=0):\n",
    "    n_batches = int(X_train.shape[0] // batch_size)\n",
    "    Xt = np.concatenate((X_train, t_train), axis=1)\n",
    "    print(f\"Number of minibatches: {n_batches}\")\n",
    "    \n",
    "    if lr_scheduler:\n",
    "        t0 = 1.; t1 = 100\n",
    "        eta = t0/t1\n",
    "        \n",
    "    print(f\"Using learning rate scheduler with initial learning rate: {eta}\")\n",
    "\n",
    "    \n",
    "    for epoch in tqdm(range(n_epoch), f\"Training {n_epoch} epochs\"):      \n",
    "        batches = np.take(Xt, np.random.permutation(Xt.shape[0]), axis=0)\n",
    "        batches = np.array_split(batches, n_batches, axis=0)\n",
    "        batch_counter = 0\n",
    "        \n",
    "        for batch in batches:\n",
    "            xi = batch[:, :-1]\n",
    "            yi = batch[:, -1].reshape(-1,1)\n",
    "\n",
    "            gradients = (2./xi.shape[0])*d_cost_MSE(xi, yi, theta, lmb)\n",
    "            theta = theta - eta*gradients\n",
    "\n",
    "            if lr_scheduler:\n",
    "                t = epoch*n_batches+epoch\n",
    "                eta = step_length(t, t0, t1)\n",
    "                #eta = learning_scheduler(eta, epoch*batch_counter)\n",
    "            \n",
    "            batch_counter += 1\n",
    "            \n",
    "    return theta.ravel()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morten SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hands_sgd(X_train, t_train, theta, n_epoch, batch_size, eta, lr_scheduler=False, lmb=0):\n",
    "    n_batches = int(X_train.shape[0] / batch_size)\n",
    "    print(f\"Number of minibatches: {n_batches}\")\n",
    "    \n",
    "    t0 = 1.; t1 = 100\n",
    "        \n",
    "    print(f\"Using learning rate scheduler with initial learning rate: {eta}\")\n",
    "    for epoch in tqdm(range(n_epoch), f\"Training {n_epoch} epochs\"):\n",
    "        for i in range(n_batches):\n",
    "            random_idx = batch_size*np.random.randint(n_batches)\n",
    "            xi = X_train[random_idx:random_idx+batch_size]\n",
    "            yi = t_train[random_idx:random_idx+batch_size]\n",
    "\n",
    "            gradients = (2./batch_size)*d_cost_MSE(xi, yi, theta, lmb)\n",
    "            eta = step_length(epoch*batch_size+i, t0, t1)\n",
    "            theta = theta - eta*gradients\n",
    "            \n",
    "    return theta.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gard SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gard_sgd(X_train, t_train, theta, n_epoch, batch_size, eta, lr_scheduler=False, lmb=0):\n",
    "    n_batches = np.ceil(X_train.shape[0] / batch_size)\n",
    "    print(f\"Number of minibatches: {n_batches}\")\n",
    "    \n",
    "    if lr_scheduler:\n",
    "        t0 = 1.; t1 = 100\n",
    "        eta = t0/t1\n",
    "        \n",
    "    print(f\"Using learning rate scheduler with initial learning rate: {eta}\")\n",
    "\n",
    "    indicies = np.arange(X_train.shape[0])\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        np.random.shuffle(indicies)\n",
    "        minibatches_idx = np.array_split(indicies, n_batches)\n",
    "        \n",
    "        for minibatch in range(len(minibatches_idx)):\n",
    "            \n",
    "            \n",
    "            xi = np.take(X_train, minibatches_idx[minibatch],axis=0)\n",
    "            yi = np.take(t_train, minibatches_idx[minibatch],axis=0).reshape(-1,1)\n",
    "\n",
    "            gradients = (2./xi.shape[0])*d_cost_MSE(xi, yi, theta, lmb)\n",
    "            theta = theta - eta*gradients\n",
    "\n",
    "            if lr_scheduler:\n",
    "                t = epoch*n_batches+epoch\n",
    "                eta = step_length(t, t0, t1)\n",
    "                #eta = learning_scheduler(eta, epoch*batch_counter)\n",
    "            \n",
    "            \n",
    "    return theta.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SEED_VALUE = 70707070\n",
    "np.random.seed(SEED_VALUE)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "t = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "X = np.hstack([X, x**2])\n",
    "X = X[:,1:]\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.2, shuffle=True)\n",
    "\n",
    "X_train, X_test = standard_scaling(X_train, X_test)\n",
    "t_train, t_test = standard_scaling(t_train, t_test)\n",
    "\n",
    "_,features_X = X_train.shape \n",
    "theta_initial_values = np.random.randn(features_X,1)\n",
    "eta = 0.01\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 5  #size of each minibatch\n",
    "lr_scheduler = True\n",
    "\n",
    "theta = hands_sgd(X_train, t_train, theta_initial_values, n_epochs, batch_size, eta, lr_scheduler=True)\n",
    "print(f\"theta from new SGD: {theta}\")\n",
    "#print(f\"Predicted value our SGD: {X_test @ theta}\")\n",
    "print(f\"MSE for SGD: {MSE(t_test, X_test @ theta)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_sgd(X_train, t_train, theta, n_epoch, batch_size, eta, beta, lr_scheduler=False, ridge=False, lmb=0):\n",
    "    n_batches = int(X_train.shape[0] // batch_size)\n",
    "    Xt = np.concatenate((X_train, t_train), axis=1)\n",
    "    print(f\"Number of minibatches: {n_batches}\")\n",
    "    \n",
    "    if lr_scheduler:\n",
    "        t0 = 1.0; t1 = 100\n",
    "        eta = t0/t1\n",
    "        print(f\"Using learning rate scheduler with initial learning rate: {eta}\")\n",
    "\n",
    "    \n",
    "    for epoch in tqdm(range(n_epoch), f\"Training {n_epoch} epochs\"):      \n",
    "        batches = np.take(Xt, np.random.permutation(Xt.shape[0]), axis=0)\n",
    "        batches = np.array_split(batches, n_batches, axis=0)\n",
    "        momentum = 0\n",
    "        \n",
    "        for batch in batches:\n",
    "            xi = batch[:, :-1]\n",
    "            yi = batch[:, -1].reshape(-1,1)\n",
    "            \n",
    "            # (2.0/n)*X.T @ (X @ beta-y)\n",
    "            gradients = 2.0 * xi.T @ ((xi @ theta)-yi)\n",
    "            \"\"\"\n",
    "            RIDGE NOT YET SUPPORTED\n",
    "            if ridge:\n",
    "                # TODO: the coff regularization is not implemented correct. \n",
    "                #gradients +=  lmb*np.eye(theta.shape[0])\n",
    "                update = lmb*np.ones(theta.shape[0]).reshape((-1,1))\n",
    "                gradients += update \n",
    "            \"\"\"\n",
    "            momentum = momentum*beta - eta*gradients\n",
    "            theta = theta + momentum\n",
    "\n",
    "            if lr_scheduler:\n",
    "                t = epoch*n_batches+epoch\n",
    "                eta = step_length(t, t0, t1)\n",
    "            \n",
    "    return theta.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMS prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(X_train, t_train, theta, n_epoch, batch_size, eta, beta, eps, lr_scheduler=False, ridge=False, lmb=0):\n",
    "    n_batches = int(X_train.shape[0] // batch_size)\n",
    "    Xt = np.concatenate((X_train, t_train), axis=1)\n",
    "    print(f\"Number of minibatches: {n_batches}\")\n",
    "    \n",
    "    if lr_scheduler:\n",
    "        t0 = 1.0; t1 = 100\n",
    "        eta = t0/t1\n",
    "        print(f\"Using learning rate scheduler with initial learning rate: {eta}\")\n",
    "\n",
    "    \n",
    "    for epoch in tqdm(range(n_epoch), f\"Training {n_epoch} epochs\"):      \n",
    "        batches = np.take(Xt, np.random.permutation(Xt.shape[0]), axis=0)\n",
    "        batches = np.array_split(batches, n_batches, axis=0)\n",
    "        \n",
    "        s = np.zeros((X_train.shape[-1],1))\n",
    "        \n",
    "        for batch in batches:\n",
    "            xi = batch[:, :-1]\n",
    "            yi = batch[:, -1].reshape(-1,1)\n",
    "            \n",
    "            # (2.0/n)*X.T @ (X @ beta-y)\n",
    "            g = 2.0* xi.T @ ((xi @ theta)-yi)\n",
    "            \"\"\"\n",
    "            RIDGE NOT YET SUPPORTED\n",
    "            if ridge:\n",
    "                # TODO: the coff regularization is not implemented correct. \n",
    "                #gradients +=  lmb*np.eye(theta.shape[0])\n",
    "                update = lmb*np.ones(theta.shape[0]).reshape((-1,1))\n",
    "                gradients += update \n",
    "            \"\"\"\n",
    "            s = s*beta + (1 - beta)*np.power(g, 2)\n",
    "            theta = theta - eta*(g/np.sqrt(s + eps))\n",
    "\n",
    "            if lr_scheduler:\n",
    "                t = epoch*n_batches+epoch\n",
    "                eta = step_length(t, t0, t1)\n",
    "            \n",
    "    return theta.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(X_train, t_train, theta, n_epoch, batch_size, eta, beta1, beta2, eps, lr_scheduler=False, ridge=False, lmb=0):\n",
    "    n_batches = int(X_train.shape[0] // batch_size)\n",
    "    Xt = np.concatenate((X_train, t_train), axis=1)\n",
    "    print(f\"Number of minibatches: {n_batches}\")\n",
    "    \n",
    "    if lr_scheduler:\n",
    "        t0 = 1.0; t1 = 100\n",
    "        eta = t0/t1\n",
    "        print(f\"Using learning rate scheduler with initial learning rate: {eta}\")\n",
    "\n",
    "    time = 0\n",
    "    for epoch in tqdm(range(n_epoch), f\"Training {n_epoch} epochs\"):      \n",
    "        batches = np.take(Xt, np.random.permutation(Xt.shape[0]), axis=0)\n",
    "        batches = np.array_split(batches, n_batches, axis=0)\n",
    "        \n",
    "        s = np.zeros((X_train.shape[-1],1))\n",
    "        m = np.zeros_like(s)\n",
    "        \n",
    "        for batch in batches:\n",
    "            time += 1\n",
    "\n",
    "            xi = batch[:, :-1]\n",
    "            yi = batch[:, -1].reshape(-1,1)\n",
    "            \n",
    "            # (2.0/n)*X.T @ (X @ beta-y)\n",
    "            g = 2.0* xi.T @ ((xi @ theta)-yi)\n",
    "            \"\"\"\n",
    "            RIDGE NOT YET SUPPORTED\n",
    "            if ridge:\n",
    "                # TODO: the coff regularization is not implemented correct. \n",
    "                #gradients +=  lmb*np.eye(theta.shape[0])\n",
    "                update = lmb*np.ones(theta.shape[0]).reshape((-1,1))\n",
    "                gradients += update \n",
    "            \"\"\"\n",
    "            m = beta1*m + (1 - beta1)*g # First moment\n",
    "            s = beta2*s + (1 - beta2)*np.power(g, 2) # Second moment\n",
    "\n",
    "            m = m / (1 - np.power(beta1, time))\n",
    "            s = s / (1 - np.power(beta2, time))\n",
    "\n",
    "            theta = theta - eta * (m / (np.sqrt(s) + eps))\n",
    "\n",
    "            if lr_scheduler:\n",
    "                t = epoch*n_batches+epoch\n",
    "                eta = step_length(t, t0, t1)\n",
    "            \n",
    "    return theta.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Terrain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4155)\n",
    "\n",
    "degree = 10\n",
    "\n",
    "X = create_X(x,y, degree)\n",
    "X = X[:,1:]\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, z_flat, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(f\"X has {X.shape[1]} features\")\n",
    "\n",
    "X_train, X_test = standard_scaling(X_train, X_test)\n",
    "t_train, t_test = standard_scaling(t_train, t_test)\n",
    "\n",
    "XT_X = X_train.T @ X_train\n",
    "H = (2./X_train.shape[0]) * XT_X\n",
    "print(f\"Upper limit learing rate: {2./np.max(np.linalg.eig(H)[0])}\")\n",
    "\n",
    "theta_initial_values = np.random.randn(X_train.shape[1],1)\n",
    "\n",
    "theta = new_sgd(X_train, t_train, theta_initial_values, 100, 5, eta=1./np.max(np.linalg.eig(H)[0]), lmb=0, lr_scheduler=True)\n",
    "print(f\"MSE for SGD: {MSE(t_test, X_test @ theta)}\")\n",
    "\n",
    "theta = hands_sgd(X_train, t_train, theta_initial_values, 100, 5, eta=1./np.max(np.linalg.eig(H)[0]), lr_scheduler=True)\n",
    "print(f\"MSE for Morten SGD: {MSE(t_test, X_test @ theta)}\")\n",
    "\n",
    "theta = gard_sgd(X_train, t_train, theta_initial_values, 100, 5, eta=1./np.max(np.linalg.eig(H)[0]), lr_scheduler=True)\n",
    "print(f\"MSE for Gard SGD: {MSE(t_test, X_test @ theta)}\")\n",
    "\n",
    "sgdreg = SGDRegressor(max_iter = 100, fit_intercept=False, penalty='l2', eta0=0.01, alpha=1, learning_rate='optimal')\n",
    "sgdreg.fit(X_train,t_train.ravel())\n",
    "print(f\"MSE for SKlearn: {MSE(t_test, X_test @ sgdreg.coef_)}\")\n",
    "\n",
    "beta_hat = np.linalg.pinv(XT_X) @ (X_train.T @ t_train)\n",
    "#print(f\"beta from inversion: {beta_hat}\")\n",
    "print(f\"MSE for OLS: {MSE(t_test, X_test @ beta_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing without regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created test data\n",
    "\"\"\"\"\"\"\n",
    "SEED_VALUE = 70707070\n",
    "np.random.seed(SEED_VALUE)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "t = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "X = np.hstack([X, x**2])\n",
    "X = X[:,1:]\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Terrain data\n",
    "\"\"\"\n",
    "degree = 1\n",
    "X = create_X(x,y, n=degree)\n",
    "X = remove_intercept(X)\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, z_flat, test_size=0.2, shuffle=True)\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test = standard_scaling(X_train, X_test)\n",
    "t_train, t_test = standard_scaling(t_train, t_test)\n",
    "\n",
    "XT_X = X_train.T @ X_train\n",
    "H = (2./n) * XT_X\n",
    "print(f\"Upper limit learing rate: {2./np.max(np.linalg.eig(H)[0])}\")\n",
    "\n",
    "_,features_X = X_train.shape \n",
    "theta_initial_values = np.random.randn(features_X,1)\n",
    "eta = 0.01\n",
    "lmb=0.0001\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 5  #size of each minibatch\n",
    "lr_scheduler = True\n",
    "\n",
    "theta = new_sgd(X_train, t_train, theta_initial_values, n_epochs, batch_size, eta, lr_scheduler=True)\n",
    "print(f\"theta from SGD: {theta}\")\n",
    "#print(f\"Predicted value our SGD: {X_test @ theta}\")\n",
    "print(f\"MSE for SGD: {MSE(t_test, X_test @ theta)}\")\n",
    "\n",
    "theta = hands_sgd(X_train, t_train, theta_initial_values, n_epochs, batch_size, eta, lr_scheduler=True)\n",
    "print(f\"theta from shuffle SGD: {theta}\")\n",
    "#print(f\"Predicted value our SGD: {X_test @ theta}\")\n",
    "print(f\"MSE for shuffle SGD: {MSE(t_test, X_test @ theta)}\")\n",
    "\n",
    "\"\"\"\n",
    "theta = momentum_sgd(X_train, t_train, theta_initial_values, n_epochs, batch_size, eta, beta=0.9, lr_scheduler=lr_scheduler)\n",
    "print(f\"theta from momentum SGD: {theta}\")\n",
    "#print(f\"Predicted value our momentum SGD: {X_test @ theta}\")\n",
    "print(f\"MSE for momentum SGD: {MSE(t_test, X_test @ theta)}\")\n",
    "\n",
    "\n",
    "theta = rmsprop(X_train, t_train, theta_initial_values, n_epochs, batch_size, eta, beta=0.9, eps=10**(-8), lr_scheduler=lr_scheduler)\n",
    "print(f\"theta from RMSprop: {theta}\")\n",
    "print(f\"MSE for RMSprop: {MSE(t_test, X_test @ theta)}\")\n",
    "\n",
    "theta = adam(X_train, t_train, theta_initial_values, n_epochs, batch_size, eta, beta1=0.9, beta2=0.99, eps=10**(-8), lr_scheduler=lr_scheduler)\n",
    "print(f\"theta from ADAM: {theta}\")\n",
    "print(f\"MSE for ADAM: {MSE(t_test, X_test @ theta)}\")\n",
    "\"\"\"\n",
    "\n",
    "beta_hat = np.linalg.pinv(XT_X) @ (X_train.T @ t_train)\n",
    "print(f\"beta from inversion: {beta_hat}\")\n",
    "print(f\"MSE for OLS: {MSE(t_test, X_test @ beta_hat)}\")\n",
    "\n",
    "sgdreg = SGDRegressor(max_iter = n_epochs, fit_intercept=False, penalty=None, eta0=eta, alpha=1, learning_rate='optimal')\n",
    "# sgdreg = SGDRegressor(max_iter = n_epochs, fit_intercept=False, penalty=None, eta0=eta)\n",
    "sgdreg.fit(X_train,t_train.ravel())\n",
    "print(f\"sgdreg from scikit: {sgdreg.coef_}\")\n",
    "#print(f\"Predicted value  SciKit: {X_test @ sgdreg.coef_}\")\n",
    "print(f\"MSE for SKlearn: {MSE(t_test, X_test @ sgdreg.coef_)}\")\n",
    "\n",
    "# print(f\"sgdreg from scikit: {sgdreg.intercept_}, {sgdreg.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing L2 regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Created test data\n",
    "\"\"\"\"\"\"\n",
    "SEED_VALUE = 70707070\n",
    "#np.random.seed(SEED_VALUE)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "t = 4+3*x+np.random.randn(n,1)\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "X = np.hstack([X, x**2])\n",
    "X = X[:,1:]\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "# Terrain data\n",
    "\"\"\"\n",
    "degree = 1\n",
    "X = create_X(x,y, n=degree)\n",
    "X = remove_intercept(X)\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, z_flat, test_size=0.2, shuffle=True)\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test = standard_scaling(X_train, X_test)\n",
    "t_train, t_test = standard_scaling(t_train, t_test)\n",
    "\n",
    "_,features_X = X_train.shape \n",
    "theta_initial_values = np.random.randn(features_X,1)\n",
    "eta = 0.01\n",
    "lmb=0.01\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 5  #size of each minibatch\n",
    "lr_scheduler = True\n",
    "\n",
    "theta = new_sgd(X_train, t_train, theta_initial_values, n_epochs, batch_size, eta, lr_scheduler=lr_scheduler, ridge=True, lmb=lmb)\n",
    "\n",
    "print(f\"theta from new SGD: {theta}\")\n",
    "print(f\"MSE for SGD with l2 regularization: {MSE(t_test, X_test @ theta)}\")\n",
    "\n",
    "\n",
    "#theta = momentum_sgd(X_train, t_train, theta_initial_values, n_epochs, batch_size, eta, beta=0.9, lr_scheduler=lr_scheduler)\n",
    "\n",
    "#print(f\"theta from momentum SGD: {theta}\")\n",
    "\n",
    "sgdreg = SGDRegressor(max_iter = n_epochs, fit_intercept=False, penalty='l2', eta0=eta, alpha=lmb, learning_rate='optimal')\n",
    "# sgdreg = SGDRegressor(max_iter = n_epochs, fit_intercept=False, penalty=None, eta0=eta)\n",
    "sgdreg.fit(X_train,t_train.ravel())\n",
    "print(f\"sgdreg from scikit: {sgdreg.coef_}\")\n",
    "print(f\"MSE for SKlearn with l2 regularization: {MSE(t_test, X_test @ sgdreg.coef_)}\")\n",
    "\n",
    "# print(f\"sgdreg from scikit: {sgdreg.intercept_}, {sgdreg.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
