# Project 2 todos

## Part a)
1. ```DONE:``` Implement SGD for OLS <br>
2. Implement SGD for Ridge <br>
3. Create function that can differ between OLS and Ridge for the gradient update <br>
4. Implement step lenght scheduler <br>
5. Add momentum to SGD <br>
6. Add Adam <br>
7. Perform an analysis of the results for OLS and Ridge regression as function of the chosen ```learning rates```, the number of ```mini-batches``` and epochs as well as algorithm for ```scaling the learning rate```. <br> 
8. You can also compare your own results with those that can be obtained using for example Scikit-Learn's various SGD options <br>
9. ```Discuss your results```. For Ridge regression you need now to study the results as functions of the hyper-parameter λ and the learning rate η. Discuss your results.
