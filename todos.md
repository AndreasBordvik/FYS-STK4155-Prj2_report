# Project 2 todos

## Part a)
DONE: Implement SGD for OLS <br>
Implement SGD for Ridge <br>
Create function that can differ between OLS and Ridge for the gradient update <br>
Implement step lenght scheduler <br>
Add momentum to SGD <br>
Add Adam <br>
Perform an analysis of the results for OLS and Ridge regression as function of the chosen 'learning rates', the number of 'mini-batches' and epochs as well as algorithm for 'scaling the learning rate'. <br> 
You can also compare your own results with those that can be obtained using for example Scikit-Learn's various SGD options <br>
'Discuss your results'. For Ridge regression you need now to study the results as functions of the hyper-parameter λ and the learning rate η. Discuss your results.

